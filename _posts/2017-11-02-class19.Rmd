---
title: "Class 19: Scraping Wikipedia Lists"
author: "Taylor Arnold"
output: html_notebook
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE)
knitr::opts_chunk$set(fig.path = "../assets/2017-11-02-class19/")
knitr::opts_chunk$set(fig.height = 5)
knitr::opts_chunk$set(fig.width = 8.5)
knitr::opts_chunk$set(out.width = "100%")
knitr::opts_chunk$set(dpi = 300)
```

```{r, message = FALSE}
library(readr)
library(ggplot2)
library(dplyr)
library(viridis)
library(stringi)
```

![](../assets/img/data_pipeline_gather.png)

## Web scraping

Today, we are going to use R to systemically build a dataset from
information on Wikipedia.

Words of caution about scraping data (note: this is not legal
advice):

- many websites prohibit the automatic downloading of material
in their terms and conditions
- a small set of people have gotten in serious trouble for scraping
websites against their terms of use
- we are only going to apply this to Wikipedia, as they have
generously permissive rules about this sort of thing
- if you ever want to replicate this on a large scale, please use
the Wikipedia API rather than the tools here

### Largest cities

Wikipedia has curated a table of the most populous cities in the
world. We can download this list, as in the last class, using
the following code:

```{r}
url <- "https://en.wikipedia.org/wiki/List_of_largest_cities"
wpage <- data_frame(line = readLines(url))
```

Make sure to view the site itself as well in a browser.

### Extracting cities

Searching through the source HTML, we see that rows in the
city table always start with the string `<th scope="row">`.
Let's filter our dataset to only include these rows:

```{r}
wpage <- filter(wpage, stri_detect(line, fixed = '<th scope="row">'))
```

We will also remove all HTML tags on these rows, and store the results as
the variable city.

```{r}
wpage <- mutate(wpage, city = stri_replace_all(line, "", regex = "<[^>]+>"))
wpage$city
```

Finally, looking at row of data, notice that there is a link to the
Wikipedia page about each city:

```{r}
wpage$line[1]
```

We can extract these using the `stri_extract` function:

```{r}
wpage <- mutate(wpage, link = stri_extract(line, regex = "/wiki/[^[\"]]+"))
wpage$link
```

### A city page

With this link, we can now download the specific information
from a given city. For instance, let's take the 5'th city
(Lagos) and download the website from Wikipedia:

```{r}
i <- 5
url <- stri_c("https://en.wikipedia.org", wpage$link[i])
cpage <- data_frame(line = readLines(url))
```

Note the use of the `stri_c` function to combine the base URL
with the link.

### Coordinates

One piece of information on most city pages is the latitude and
longitude of the city. We can find this by detecting the string
"Coordinates:" in the webpage. Here, we save only the first
occurrence of the string (here there is only one, but we want
to safely abstract this to other pages):

```{r}
coord <- filter(cpage, stri_detect(line,
                          fixed = "Coordinates:"))$line[1]
coord
```

Looking at the output, we want to not just remove the HTML tags but
to capture a very specific part of the tags. Here we get the easiest
form of the coordinates from the "geo" tag:

```{r}
coord <- stri_extract(coord,
                      regex = "<span class=\"geo\">[^<]+</span>")
coord
```

We want to now remove the html tags. We could do this by a regular expression,
but let's instead do it by taking a substring:

```{r}
coord <- stri_sub(coord, 19, -8)
coord
```

Next, we want to split the string into two parts based on the ";" symbol.
We do this with the `stri_split` function.

```{r}
coord <- stri_split(coord, fixed = ";")[[1]]
coord
```

Finally, we want to convert these strings into numbers. Recall that
`factor` turns numeric data into categorical ones; at the time, I
mentioned that rarely can we go in the reverse order. Here we use
the `as.numeric` function to do just that:

```{r}
coord <- as.numeric(coord)
lat <- coord[1]
lon <- coord[2]
lat
lon
```

### Country name

Let's also find the name of the country that each city is in.
this is a bit complicated because the row that shows the tag
"Country" is actually one away from the row that gives the
data:

```{r}
filter(cpage, stri_detect(line,
               fixed = "<th scope=\"row\">Country</th>"))
```

To fix this, wrap the `stri_detect` function in the function `which`. This
gives the indices where something is True:

```{r}
id <- which(stri_detect(cpage$line,
             fixed = "<th scope=\"row\">Country</th>"))
id
```

Now, we just add 1 to this index (extracting just the first one, in case there
are multiple) and grab those lines:

```{r}
country <- cpage$line[id[1] + 1]
country
```

Cleaning up the results yields the desired information:

```{r}
country <- cpage$line[which(stri_detect(cpage$line,
            fixed = "<th scope=\"row\">Country</th>"))[1] + 1]
country <- stri_replace_all(country, "", regex = "<[^>]+>")
country <- stri_replace_all(country, "", fixed = "&#160;")
country
```

### Automating over cities

We now have code that works on a single city page. Let's add empty
attributes to the `wpage` dataset. We'll fill these in in a moment.

```{r}
wpage$lat <- NA
wpage$lon <- NA
wpage$country <- NA
```

We now use a *for loop* to cycle over all rows of the `wpage` dataset.
A simple template describing how the loop works looks like the block
belove, with the extraction code filled in at the comment:

```{r, eval = FALSE}
for (i in 1:nrow(wpage)) {
  url <- stri_c("https://en.wikipedia.org", wpage$link[i])
  cpage <- data_frame(line = readLines(url))

  # extract stuff

  wpage$lat[i] <- lat
  wpage$lon[i] <- lon
  wpage$country[i] <- country
}
```

Here is the full code, with the details filled in:

```{r, eval = FALSE}
for (i in 1:nrow(wpage)) {
  url <- stri_c("https://en.wikipedia.org", wpage$link[i])
  cpage <- data_frame(line = readLines(url))

  coord <- filter(cpage,
      stri_detect(line, fixed = "Coordinates:"))$line[1]
  coord <- stri_extract(coord,
      regex = "<span class=\"geo\">[^<]+</span>")
  coord <- stri_sub(coord, 19, -8)
  coord <- stri_split(coord, fixed = ";")[[1]]
  coord <- as.numeric(coord)
  wpage$lat[i] <- coord[1]
  wpage$lon[i] <- coord[2]

  country <- cpage$line[which(stri_detect(cpage$line,
      fixed = "<th scope=\"row\">Country</th>"))[1] + 1]
  country <- stri_replace_all(country, "", regex = "<[^>]+>")
  country <- stri_replace_all(country, "", fixed = "&#160;")

  wpage$country[i] <- country
}
```

Running that takes several minutes and I do not want to ping
the Wikipedia server more than necessary. Here, I will just
load the final results, which I have loaded to the class 
website:

```{r, echo = FALSE, eval = TRUE, message = FALSE}
wpage <- read_csv("https://statsmaths.github.io/stat289/assets/data/wpage_cities.csv")
wpage
```

We can now plot the data as follows:

```{r, echo = FALSE, eval = TRUE, message = FALSE}
qplot(lon, lat, data = wpage, label = city, geom = "blank") +
  geom_text(size = 2.5) +
  theme_minimal()
```

We can label the most common countries by using the
`fct_lump` function from the **forcats** package:

```{r, echo = FALSE, eval = TRUE, message = FALSE}
library(forcats)
qplot(lon, lat, data = wpage, label = city, geom = "blank") +
  geom_point(aes(color = fct_lump(country, 4))) +
  theme_minimal()
```

Or, simply tabulate by country:

```{r}
count(wpage, country, sort = TRUE)
```

## Shortcuts with XML

The techniques above are very useful because they can be used to
extract meaningful data from arbitrary textual sources. When 
working with properly formed HTML or XML, it is possible to 
short cut many of these tricks by using a proper XML parser. The
**XML** package in R provides full-feature XML parser; it is
fantastic but would require a dedicated class or more to work
with so I am not going to cover it here. Here I will describe
just one simple function from the package that is particularly
useful.

Note that you'll need to install **XML** if you want
to use it as it was not included in the `setup.R` script we
had in the first week of class. Let's take a look
at the following Wikipedia page:

- [city tempuratures](https://en.wikipedia.org/wiki/List_of_cities_by_temperature)

There are a number of tables on this page. We can extract
these all as a list of R data frames using the `readHTMLTable`
function:

```{r, warning = FALSE}
library(XML)
url <- "https://en.wikipedia.org/wiki/List_of_cities_by_temperature"
tabs <- readHTMLTable(readLines(url), stringsAsFactors = FALSE)
length(tabs)
```

The six tables represent the six regions that the cities are broken
up into. Let's take the second element and convert it into a **dplyr**
data tibble:

```{r}
as_data_frame(tabs[[2]])
```

There is still some cleaning up needed here, but the `readHTMLTable`
function has saved us a lot of time in getting this far. A commonly
needed tweak is to access the hyperlinks within the table (they are
removed in the text above). Here, I define a custom parsing function
that returns the first link found in each cell:

```{r, warning = FALSE}
hrefFun <- function(x){
  res <- xpathSApply(x,'./a/@href')[1]
  ifelse(!is.null(res), res, NA_character_)
}

tabs <- readHTMLTable(readLines(url), elFun = hrefFun, stringsAsFactors = FALSE)
as_data_frame(tabs[[2]])
```

You could then follow the links in the table and scrape those. This is the
sort of thing you will be doing for Project II.

