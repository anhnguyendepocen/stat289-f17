---
title: "Class 07: Regression Models"
author: "Taylor Arnold"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE)
knitr::opts_chunk$set(fig.path = "../assets/2017-09-19-class07/")
knitr::opts_chunk$set(fig.height = 5)
knitr::opts_chunk$set(fig.width = 8.5)
knitr::opts_chunk$set(out.width = "100%")
knitr::opts_chunk$set(dpi = 300)
```

```{r, message = FALSE}
library(readr)
library(ggplot2)
library(dplyr)
library(viridis)
```

![](../assets/img/data_pipeline_model.png)

## Models

Today we turn our attention to modeling, which complements
the graphical and data manipulation techniques that we have
so far covered.

The first model we will look at is linear regression. You have,
in fact, already seen this graphically when fitting a smoothing line
to data using `geom_smooth("lm")`. Today we will start to see how to
implement this model directly in R and how to analyze how well it fits
the data at hand.

There are three basic tasks of modelling in statistics:

- exploratory: understanding the distributions and
relationships between variables
- inferential: formal statistical hypothesis tests and
confidence intervals
- prediction: building models to predict new outputs

One reason for the popularity of linear regression models is
that they are excellent choices for all three types of tasks.
We'll cover aspects of all three today.

## Simple Linear Models

### Tea data

We are going to look at a dataset of teas. Specifically,
tea reviews from the [Adagio Tea](http://www.adagio.com/) website.
I collected this dataset about 12 months ago, so it should be
similar but not exactly the same as what is one the site today.
Let's read the data into R from my website as we have done with
the mammals and abalone datasets:

```{r, message = FALSE}
tea <- read_csv("https://statsmaths.github.io/stat_data/tea.csv")
```

Opening the data in the data viewer, we see the variable `score`,
which is the primary variable we would like to understand. That is,
what makes a tea that people like?

Variables available to predict the output are the type of tea,
the number of reviews received the price of the tea. The latter
is given in estimated cents per cup as reported on the site.
We also have the full name of the tea.

There seems to be a slight positive relationship between the number of
reviews and the score.

```{r, warning=FALSE}
ggplot(tea, aes(price, score)) +
  geom_point()
```

Given the skew of the plot, it is hard to figure out the exact relationship
between price and score.

We have used the `geom_smooth` function to put what I
called a best-fit line onto the plot.
Let's try to formalize this concept now.

### Ordinary least squares

The classical linear regression model assumes that the
average value, or mean, of the response Y is a linear
function of X. Symbolically, with the index i representing
the i'th sample, this gives:

$$ \text{mean} (Y_i) = \alpha + \beta * X_i $$

Similarly, we can write that Y is equal to a fixed linear
effect dependent on X plus a random variable epsilon with
zero mean.

$$ Y_i = \alpha + \beta * X_i + \epsilon_i, \quad mean(\epsilon_i) = 0 $$

The estimation task here is to find reasonable estimates for
the alpha and beta components given pairs of observations (X, Y).
There are many ways of doing this but by far the most common is
to use what is known as **Ordinary Least Squares** or OLS.
This selects the alpha and beta that minimize the squared errors
implied by the linear model. As before, let's write this down
symbolically:

$$ \alpha, \beta \in \arg\min \left\{ \left. \sum_i \left(y_i - a - b x_i \right)^2 \quad \right| \quad a, b \in \mathbb{R} \right\} $$

A natural question is to ask why we are interested in the squared
errors. We could just as likely ask for the minimizer of the
absolute value of the errors or the minimizer of the maximum
errors.

I can offer two justifications of the squared error, one numerical
and one statistical. Using the squared error, unlike the absolute
errors or maximum errors, produces a smooth function. That is, a
function that has an infinite number of derivatives at every point
(although, all derivatives after 2 are equal to zero). This makes it
easier to find a solution to the linear equation. Secondly, if the
errors are distributed as a normal random variable the OLS estimator
is equivalent to the maximum likelihood estimator (MLE).

### Linear Models - Visually

In order to better understand linear models, it helps to see a
picture. Below I have drawn a line through our dataset and
indicated the errors (also known as residuals) that the ordinary
least squares is concerned with minimizing.

```{r, warning = FALSE}
tea <- mutate(tea, score_pred = 89 + num_reviews * 0.002)
ggplot(tea, aes(num_reviews, score)) +
  geom_smooth(method = "lm") +
  geom_point() +
  geom_segment(aes(xend = num_reviews, yend = score_pred))
```

Notice that this line under-guesses most of the score of teas, particularly
if the number of reviews is low.


## Using lm

Now, let's compute the best fit line using the lm function:

```{r, warning = FALSE}
model <- lm(score ~ num_reviews, data = tea)
model
```

Let's use the predict function to fit this model
to our data:

```{r}
tea <- mutate(tea, score_pred = predict(model))
```

And now plot the fitted values. Does it visually
correspond to where you would expect the best fit
line to run?

```{r}
ggplot(tea, aes(num_reviews, score_pred)) +
  geom_smooth(method = "lm") +
  geom_point() +
  geom_segment(aes(xend = num_reviews, yend = pred))
```

## Evaluating models

How should be evaluate the predictive ability of a model?
The method of ordinary least squares suggests that we
should consider the sum of squared errors. A challenge with
this is that the value of a sum will grow in proportion to
how many observations there are. This makes it hard to compare
results across data sources and subsets. A simple solution
is to use the average value of the squared errors, known as
the mean squared error or MSE:

```{r, warning=FALSE}
mean((tea$score - tea$score_pred)^2, na.rm = TRUE)
```

This works as a general measurement of error, but the units
are a bit strange as they are given in squared scores. A
simple solution to this exists by taking the square root
of the MSE, resulting in the root mean squared error (RMSE):

```{r, warning=FALSE}
sqrt(mean((tea$score - tea$score_pred)^2, na.rm = TRUE))
```

You'll find references to the RMSE through the literature on
machine learning as it is by far the most common measurement
for productiveness of continuous responses.

## Multivariate linear regression

The linear regression model I just introduced is known
as simple linear regression because there is only one
explanatory variable. We can easy consider multivariate
models; for instance, we can be write a two variable
model mathematically as follows:

$$ Y_i = \alpha + \beta * X_i + \gamma * Z_i + \epsilon_i, \quad mean(\epsilon_i) = 0 $$

The geometric interpretation of this is that we have plane
in place of the line in the simple linear regression model.

Each slope coefficient (beta and gamma here) corresponds to a
weight placed on how much the response changes with each
predictor variable.

Fitting multivariate models is also quite easy with the
`lm` function. Simply add the variables together that
you would like to use for prediction. Here we use both
the number of reviews and the price of the tea:

```{r}
model <- lm(score ~ num_reviews + price, data = tea)
tea <- mutate(tea, score_pred = predict(model))
model
```

Using the color aesthetic we
can visualize the predicted values of this model. Don't
get hung up on the code below; concentrate on the output.

```{r}
tea_grid <- expand.grid(seq(min(tea$num_reviews), max(tea$num_reviews), length.out = 20),
                        seq(min(tea$price), max(tea$price), length.out = 20))
tea_grid <- data_frame(num_reviews = tea_grid[,1],
                       price = tea_grid[,2])
tea_grid$score_pred <- predict(model, newdata = tea_grid)
ggplot(tea_grid, aes(price, num_reviews)) +
  geom_point(aes(color = score_pred), size = 5) +
  scale_color_viridis() +
  theme_minimal()
```

Also, the productiveness of the model has been
greatly improved over the simple linear regression from
before:

```{r}
sqrt(mean((tea$score - tea$score_pred)^2, na.rm = TRUE))
```

## Interpreting Regression Models

The interpretation of a regression model can be described succinctly in
the language of calculus. The slopes correspond to how much we expect the
mean of the response to change as an individual predictor changes, with
all other predictors held fixed. The exact interpretation of these
models is very difficult when there are more than a few numeric variables.
Generally I find them useful for describing patterns in the data but
find that one should not rely too closely on the exact output

## Inference for Regression Models

You should all be familiar with the structure of statistical hypothesis
testing and confidence intervals. If not, the following is a good sources
for a brief review or catch-up:

- [*Nature* Points of Significance: Significance, P values
and t-tests](https://www.nature.com/nmeth/journal/v10/n11/pdf/nmeth.2698.pdf)

We can easily get p-values that test whether each coefficient in a
regression model is zero by calling the `summary` function on the
model output of `lm`:

```{r}
model <- lm(score ~ num_reviews + price, data = tea)
summary(model)
```

The tests are all T-tests; the t-score as well as the standard errors
are included in the table at well. We need more probability theory
than we have time to cover here to explain the detailed assumptions
that make these p-values valid, but approximately we have the following:

- the actual model is linear
- the samples are all independent
- the predictor values are "full rank" (you cannot write any
predictor as a linear combination of the others)
- the x-values are either non-random or independent of the errors

Similarly, we can access confidence intervals for the linear model
using the `confint` function:

```{r}
model <- lm(score ~ num_reviews + price, data = tea)
confint(model, level = 0.95)
```

The assumptions are the same as those for the hypothesis tests.

I am personally quite sceptical of p-values from large linear models.
Here is an excellent demo showing how easy *p-hacking* is:

[Hack Your Way To Scientific Glory](https://fivethirtyeight.com/features/science-isnt-broken/#part1)

In general, in order to be valid hypothesis tests, we need to decide
on the exact model before seeing any of the data and only run one
model on the dataset. While we can run more models and modify them
as we observe results, these can only be used for exploratory purposes.



