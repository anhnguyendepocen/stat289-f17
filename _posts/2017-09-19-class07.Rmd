---
title: "Class 07: Reading Data and Statistical Tests"
author: "Taylor Arnold"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE)
knitr::opts_chunk$set(fig.path = "../assets/2017-09-19-class07/")
knitr::opts_chunk$set(fig.height = 5)
knitr::opts_chunk$set(fig.width = 8.5)
knitr::opts_chunk$set(out.width = "100%")
knitr::opts_chunk$set(dpi = 300)
```

```{r, message = FALSE}
library(readr)
library(ggplot2)
library(dplyr)
library(viridis)
```

![](../assets/img/data_pipeline_model.png)

## Statistical Tests

Today we turn our attention to modeling, which complements
the graphical and data manipulation techniques that we have
so far covered.

You should all be familiar with the structure of statistical hypothesis
testing and confidence intervals. If not, the following is a good sources
for a brief review or catch-up:

- [*Nature* Points of Significance: Significance, P values
and t-tests](https://www.nature.com/nmeth/journal/v10/n11/pdf/nmeth.2698.pdf)

Today I will very quickly review this material in the context of the dataset
we just constructed.

### One-sample inference

Let's look at the mpg data set once again.

```{r, message = FALSE}
mpg <- read_csv("https://statsmaths.github.io/stat_data/mpg.csv")
```

Assuming that this was constructed as a random sample from all available
car models, we could construt a confidence interval for what the mean
city fuel efficency of all cars on the market are by using the function
`t.test`.

```{r}
t.test(mpg$cty)
```

We see that the 95% confidence interval is from 16.31 to 17.41
miles per gallon. If this is actually a uniform random sample from
some population, and the true distribution is not too highly
skewed, this procedure is guaranteed to give a range containing
the true mean 95% of the time.

The `t.test` function also runs an hypothesis test for us. If the
true average value of city fuel efficency was 0 miles per gallon,
we would have observed a sample with this large a sample mean less
that 2.2e-16 of the time (the p-value above). Of course, testing
for fuel efficency being zero is ridiculous. It has to be a non-negative
value for any functioning car. Let's instead test for how rare these
results are if the "true mean" was 16:

```{r}
t.test(mpg$cty, mu = 16)
```

Here, we would expect a sample as *extreme* as this one only 0.226% of 
the time if the true mean were 16 mile per gallon.

### Two-sample inference

We can also use the `t.test` function to compare the means of
two different groups. For example, is there a difference between
the city fuel efficency of a randomly choosen Subaru car and a randomly
choosen Toyota car. We can make two different datasets to compare
this using our `filter` function:

```{r}
subaru <- filter(mpg, manufacturer == "subaru")
toyota <- filter(mpg, manufacturer == "toyota")
```

And then passing both variables to the `t.test` function:

```{r}
t.test(subaru$cty, toyota$cty)
```

Here, the default null-hypothesis is that there is not difference
between the two fuel efficencies. We see that the p-value is only
0.31, so 31% of the time we would get these results if there is
no difference between the fuel efficency of a randomly choosen
Toyota automobile and randomly choosen Subaru automobile. The
confidence interval for the difference in fuel efficencies is
-0.73 to 2.24 miles per gallon. Notice that this range includes
zero, indicating our lack of confidence in there being any difference
at all.

### Simple linear regression

Let's assume that there is a linear relationship between the city
fuel efficency and highway fuel efficency of any randomly choosen
automobile. Specifically, assume that:

$$ \text{cty}_i = \alpha + \beta \cdot \text{hwy}_i + \epsilon_i$$

Where alpha and beta are constant and epsilon is some random noise
that has a zero mean, is not correlated with the highway fuel efficency,
and is independent for each sampled car. We can represent this
relationship graphically with our tools at hand:

```{r}
ggplot(mpg, aes(hwy, cty)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_minimal()
```

We can model this more formally in R with the `lm` function:

```{r}
model <- lm(cty ~ hwy, data = mpg)
model
```

The model object gives us the predicted slope and intercept. More
on this later. If we apply the `summary` function (not `summarize`),
we will get T-scores and p-values, which run hypothesis test for each
coefficents being equal to zero:

```{r}
summary(model)
```

And the `confint` function gives confidence intervals for each
parameter.

```{r}
confint(model)
```


![](../assets/img/data_pipeline_collect.png)

## Data Collection

Today's lab has you create a small dataset and apply these modelling
techniques on your dataset. Start by opening
some spreadsheet software on your machine. This can be Excel, GoogleSheets,
OpenOffice, or anything else that you are comfortable with.

Create a new spreadsheet. The first step in collecting data is decided
what variables are going to be collected and what we want to call them.
Here, we are going to collect data about the number of emails that you
receive each day in your University of Richmond email account. The variables
we want to store are the following

- **date**: the date of the observation, in YYYY-MM-DD format
- **total_count**: the total number of e-mail chains you received
- **self_count**: the total number of e-mail chains you received, when
filtering only to those that used your first name
- **week_num**: the week of the semester, either 2 or 3
- **dow**: the day of the week; equal to 1 for Monday, 2 for Tuesday, ...,
and 7 for Sunday

It will be easiest, given that you all have GMail accounts, to tabulate
e-mail chains rather than individual e-mails. Most chains all occur on
the same day anyway, so this should not be a problem. 

We are going to collect data from 2017-09-04 (Monday)
through 2017-08-17 (this Sunday). This gives two complete weeks of data.

The best way to start collecting this data is to fill in all of the dates
(I recommend working backwards; that's how your e-mail is shown) and the
easy variables like **week_num** and **dow** that do not require looking at
your e-mail. Then, go through and quickly count the number of messages in
each day. Finally, search for your first name, and repeate the process.

## Data Export and Import

Once you have finished inputing the dataset, export the document as a csv
file. I recommend saving it as "email_data.csv" and storing in on your
desktop. Then, go into R and read the dataset in with the `read_csv` function.
You may need to change this if you saved the file:

```{r, eval = FALSE}
email <- read_csv("email_data.csv")
```

Make sure that all of the variables loaded in correctly and have the correct
data type.

We are going to use the data you just constructed to investigate three
questions:

- *What is the mean number of emails you have in a day?*
- *Do you receive more emails in the first half of the week (Monday-Tuesday)
compared to the second (Wednesday - Friday)?*
- *What is the linear relationship between the total number of
emails you receive in a day and the total number of email chains
you receive in a day that use your first name?*

There is nothing particularly tricky here. Just apply the three tests
I have mentioned in the notes today.




