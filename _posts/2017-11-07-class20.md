---
title: "Class 20: Working with Tokens"
author: "Taylor Arnold"
output: html_notebook
---





{% highlight r %}
library(readr)
library(ggplot2)
library(dplyr)
library(viridis)
{% endhighlight %}

![](../img/data_pipeline_transform.png)

## NLP

We have been using basic string processing functions in **stringi**
to perform basic web scraping and data manipulation tasks.

Today, we extend these ideas by using the **tidytext** package to
parse the actual text and extract meaningful information from it.

## tidytext

The basic idea of cleanNLP is to turn text into a data frame with
one row per word. The basic usage is as follows:


{% highlight r %}
library(tidytext)

input <- data_frame(text = c("Penguins are awesome.",
                             "Birds that can swim!"),
                    id = c("p1", "p2"))
data <- unnest_tokens(input, word, text)
{% endhighlight %}

## tidytext


{% highlight r %}
data
{% endhighlight %}



{% highlight text %}
## # A tibble: 7 x 2
##      id     word
##   <chr>    <chr>
## 1    p1 penguins
## 2    p1      are
## 3    p1  awesome
## 4    p2    birds
## 5    p2     that
## 6    p2      can
## 7    p2     swim
{% endhighlight %}

## website text

Previously, when we scrapped data from Wikipedia we did
not do anything with the raw text (though you should have
in one of the associated labs). Here is how we could have
grabbed the text from the Lagos page:


{% highlight r %}
url <- "https://en.wikipedia.org/wiki/Lagos"
wpage <- data_frame(line = readLines(url))
wpage <- filter(wpage, stri_detect(line, fixed = "<p>"))
wpage <- mutate(wpage,
        line = stri_replace_all(line, "", regex = "<[^>]+>"))
{% endhighlight %}

## website text

Here is the result (using `stri_wrap` just for display
purposes):


{% highlight r %}
text <- stri_flatten(wpage$line, collapse = " ")
stri_wrap(text)[1:10]
{% endhighlight %}



{% highlight text %}
##  [1] "Lagos /ˈleɪɡɒs/[11] (Yoruba: Èkó) is a city in the Nigerian state"  
##  [2] "of Lagos. The city, with its adjoining conurbation, is the largest" 
##  [3] "in Nigeria, as well as on the African continent. It is one of the"  
##  [4] "fastest growing in the world,[12][13][14][15][16][17][18] and also" 
##  [5] "one of the most populous urban agglomerations.[19][20] Lagos is a"  
##  [6] "major financial centre in Africa; the megacity has the highest GDP,"
##  [7] "[4] and also houses one of the largest and busiest ports on the"    
##  [8] "continent.[21][22][23] Lagos initially emerged as a port city which"
##  [9] "originated on a collection of islands, which are contained in the"  
## [10] "present day Local Government Areas (LGAs) of Lagos Island, Eti-"
{% endhighlight %}

## website text

Let's build a small example with just three cities:


{% highlight r %}
urls <- c("https://en.wikipedia.org/wiki/Lagos",
          "https://en.wikipedia.org/wiki/London",
          "https://en.wikipedia.org/wiki/Saint_Petersburg")

df <- data_frame(city = c("Lagos", "London",
                          "Saint Petersburg"))
df$text <- NA
{% endhighlight %}

## website text

And cycle over these to extract a text column in our
dataset:


{% highlight r %}
for (i in 1:3) {
    wpage <- data_frame(line = readLines(urls[i]))
    wpage <- filter(wpage, stri_detect(line, fixed = "<p>"))
    wpage <- mutate(wpage,
        line = stri_replace_all(line, "", regex = "<[^>]+>"))

    df$text[i] <- stri_flatten(wpage$line, collapse = " ")
}
stri_length(df$text)
{% endhighlight %}



{% highlight text %}
## [1] 31972 78529 72576
{% endhighlight %}

## tidytext

With **tidytext**, we can extract the tokens from these three
pages, keeping the city name intact:


{% highlight r %}
tokens <- unnest_tokens(df, word, text)
tokens
{% endhighlight %}



{% highlight text %}
## # A tibble: 29,775 x 2
##     city    word
##    <chr>   <chr>
##  1 Lagos   lagos
##  2 Lagos ˈleɪɡɒs
##  3 Lagos      11
##  4 Lagos  yoruba
##  5 Lagos     èkó
##  6 Lagos      is
##  7 Lagos       a
##  8 Lagos    city
##  9 Lagos      in
## 10 Lagos     the
## # ... with 29,765 more rows
{% endhighlight %}


## top tokens

Let's use our new grouping function to find the top words
in each city page:


{% highlight r %}
temp <- group_by(tokens, city, word)
temp <- count(temp, city)
temp <- group_by(temp, city)
top_n(temp, n = 3)
{% endhighlight %}



{% highlight text %}
## Selecting by n
{% endhighlight %}



{% highlight text %}
## # A tibble: 9 x 3
## # Groups:   city [3]
##               city   word     n
##              <chr>  <chr> <int>
## 1            Lagos  lagos   199
## 2            Lagos     of   198
## 3            Lagos    the   417
## 4           London london   410
## 5           London     of   531
## 6           London    the  1053
## 7 Saint Petersburg    and   394
## 8 Saint Petersburg     of   500
## 9 Saint Petersburg    the  1024
{% endhighlight %}

## stop words

Two of the city names pop up, but the other words are just
common, boring English terms. We can use a stopword list
to remove these.

## stop words


{% highlight r %}
stop_words
{% endhighlight %}



{% highlight text %}
## # A tibble: 1,149 x 2
##           word lexicon
##          <chr>   <chr>
##  1           a   SMART
##  2         a's   SMART
##  3        able   SMART
##  4       about   SMART
##  5       above   SMART
##  6   according   SMART
##  7 accordingly   SMART
##  8      across   SMART
##  9    actually   SMART
## 10       after   SMART
## # ... with 1,139 more rows
{% endhighlight %}

## top tokens

The `anti_join` function returns the first dataset with
all rows matching the second removed.


{% highlight r %}
temp <- anti_join(tokens, stop_words)
{% endhighlight %}



{% highlight text %}
## Joining, by = "word"
{% endhighlight %}



{% highlight r %}
temp <- group_by(temp, city, word)
temp <- count(temp, city)
temp <- group_by(temp, city)
top_n(temp, n = 3)
{% endhighlight %}



{% highlight text %}
## Selecting by n
{% endhighlight %}



{% highlight text %}
## # A tibble: 9 x 3
## # Groups:   city [3]
##               city       word     n
##              <chr>      <chr> <int>
## 1            Lagos       city    33
## 2            Lagos     island    35
## 3            Lagos      lagos   199
## 4           London        160    71
## 5           London       city   119
## 6           London     london   410
## 7 Saint Petersburg       city   142
## 8 Saint Petersburg petersburg   177
## 9 Saint Petersburg      saint   166
{% endhighlight %}

## custom stop words

We can build a better list using our data itself:


{% highlight r %}
temp <- group_by(tokens, word)
temp <- ungroup(count(temp))
custom_stop_words <- top_n(temp, n = 300)
{% endhighlight %}



{% highlight text %}
## Selecting by n
{% endhighlight %}



{% highlight r %}
custom_stop_words
{% endhighlight %}



{% highlight text %}
## # A tibble: 325 x 2
##     word     n
##    <chr> <int>
##  1     1    16
##  2   100    12
##  3    12    16
##  4    15    12
##  5    16    13
##  6   160   156
##  7  18th    12
##  8  19th    14
##  9  2006    16
## 10  2007    12
## # ... with 315 more rows
{% endhighlight %}

## top tokens

Which yields these much improved results:


{% highlight r %}
temp <- anti_join(tokens, custom_stop_words)
{% endhighlight %}



{% highlight text %}
## Joining, by = "word"
{% endhighlight %}



{% highlight r %}
temp <- group_by(temp, city, word)
temp <- count(temp, city)
temp <- group_by(temp, city)
{% endhighlight %}

## top tokens


{% highlight r %}
top_n(temp, n = 3)
{% endhighlight %}



{% highlight text %}
## Selecting by n
{% endhighlight %}



{% highlight text %}
## # A tibble: 20 x 3
## # Groups:   city [3]
##                city       word     n
##               <chr>      <chr> <int>
##  1            Lagos     africa     9
##  2            Lagos      ikoyi     9
##  3            Lagos   nigerian    10
##  4           London     celtic     8
##  5           London       deer     8
##  6           London       fire     8
##  7           London     forest     8
##  8           London      found    10
##  9           London    gallery     8
## 10           London    kingdom     8
## 11           London      outer     8
## 12           London    outside     8
## 13           London      roman     9
## 14 Saint Petersburg    finland    10
## 15 Saint Petersburg historical    10
## 16 Saint Petersburg  monuments    10
## 17 Saint Petersburg     oblast    11
## 18 Saint Petersburg   prospekt    10
## 19 Saint Petersburg       ussr    10
## 20 Saint Petersburg     winter    11
{% endhighlight %}

## idea for using this

Here are some ways that you can use this in your
data analysis projects:

- find top word or words for each location and plot on a map
- count number of words in each page and use this as metadata
- create a list of interesting words and use `semi_join` (the
opposite of `anti_join`) to filter only those words that
are on this list


