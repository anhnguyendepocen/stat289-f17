---
title: "Class 08: Extending Multivariate Models"
author: "Taylor Arnold"
output: html_notebook
---




{% highlight r %}
library(readr)
library(ggplot2)
library(dplyr)
library(viridis)
{% endhighlight %}

![](../assets/img/data_pipeline_model.png)


## Multivariate regression with categorical data

### Cars Dataset

Today we are, once again, going to look at another classic datasets
in statistics featuring data about a number of automobiles.


{% highlight r %}
cars <- read_csv("https://statsmaths.github.io/stat_data/mpg.csv")
{% endhighlight %}

Our goal today is to estimate the city fuel efficiency of each car.

### Categorical predictors

It would be reasonable to start with a regression model
that uses `displ` to predict the response variable.
We can just as easily add categorical data into our
model. Next week we will cover the specifics of what
is internally being done here, but for now let's just
see what adding the `class` variable to the model does
to the output:


{% highlight r %}
model <- lm(cty ~ displ + class,
            data = cars)
cars$score_pred <- predict(model, newdata = cars)
model
{% endhighlight %}



{% highlight text %}
## 
## Call:
## lm(formula = cty ~ displ + class, data = cars)
## 
## Coefficients:
##     (Intercept)            displ     classcompact     classmidsize  
##          28.777           -2.172           -3.599           -3.676  
##    classminivan      classpickup  classsubcompact         classsuv  
##          -5.595           -6.182           -2.629           -5.599
{% endhighlight %}

Notice that it appears that we now have a separate term for
each class of car. If you look more carefully you'll see that
there is no mention of "2seater" in the list. This value is
excluded because otherwise we would have perfect collinearity
between the variables (a violation of the model assumptions)

## Extensions to the linear model

### Generalized linear models

There are many other functions in R that have similar calling
mechanisms to `lm` but run different underlying models.

For example, `glm` fits generalized linear models. These can
be used, amongst other things, for fitting a model to a binary
response:


{% highlight r %}
df <- data_frame(y = c(0,0,0,0,1,1,1,1), x = rnorm(8))
model <- glm(y ~ x, data = df, family = binomial())
summary(model)
{% endhighlight %}



{% highlight text %}
## 
## Call:
## glm(formula = y ~ x, family = binomial(), data = df)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.3085  -1.1577   0.0126   1.0724   1.5010  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)
## (Intercept)  -0.2570     0.8097  -0.317    0.751
## x             0.7186     0.8329   0.863    0.388
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 11.090  on 7  degrees of freedom
## Residual deviance: 10.214  on 6  degrees of freedom
## AIC: 14.214
## 
## Number of Fisher Scoring iterations: 4
{% endhighlight %}

### Robust linear models

In the **MASS** package (included with all standard R
installations) is the `rlm` function for fitting robust
linear regression:


{% highlight r %}
library(MASS)
{% endhighlight %}



{% highlight text %}
## 
## Attaching package: 'MASS'
{% endhighlight %}



{% highlight text %}
## The following object is masked from 'package:dplyr':
## 
##     select
{% endhighlight %}



{% highlight r %}
x <- rnorm(100)
y <- 1 + x + rnorm(100, sd = 0.2)
y[50] <- 1e4
model_lm <- lm(y ~ x)
model_rlm <- rlm(y ~ x)
{% endhighlight %}

We see that the robust version is much more accurate than
the standard regression function in this case:


{% highlight r %}
summary(model_lm)
{% endhighlight %}



{% highlight text %}
## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -297.1 -159.6 -100.7  -44.5 9839.7 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)
## (Intercept)    83.12     102.82   0.808    0.421
## x             -77.46     101.21  -0.765    0.446
## 
## Residual standard error: 1002 on 98 degrees of freedom
## Multiple R-squared:  0.005941,	Adjusted R-squared:  -0.004203 
## F-statistic: 0.5857 on 1 and 98 DF,  p-value: 0.4459
{% endhighlight %}



{% highlight r %}
summary(model_rlm)
{% endhighlight %}



{% highlight text %}
## 
## Call: rlm(formula = y ~ x)
## Residuals:
##        Min         1Q     Median         3Q        Max 
## -4.720e-01 -1.571e-01 -1.513e-02  1.477e-01  1.000e+04 
## 
## Coefficients:
##             Value   Std. Error t value
## (Intercept)  0.9871  0.0214    46.0942
## x            0.9936  0.0211    47.1332
## 
## Residual standard error: 0.233 on 98 degrees of freedom
{% endhighlight %}

### Other models

If you have a need for a specific model, you can usually
find an R package that support it. In most cases, the model
will roughly resemble calling `lm`.

Some common examples you may run into:

- `gam::gam` for generalized additive models
- `nls` for non-linear regression
- `lme4::lmer` for mixed effects models
- `quantreg::qr` for quantila regression
- `glmnet::glmnet` for the generalized elastic net
- `randomforest::randomforest` for random forest classifications
- `forcast::auto.arima` for modeling time series



