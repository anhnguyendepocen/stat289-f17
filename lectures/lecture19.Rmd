---
title: "Lecture 19: Scraping Wikipedia Lists"
author: "Taylor Arnold"
date: " "
fontsize: 9pt
output:
  beamer_presentation:
    template: template.beamer
    pandoc_args: "--latex-engine=xelatex"
    theme: "metropolis"
    fonttheme: "professionalfonts"
    slide_level: 2
    df_print: tibble
    fig_width: 12
    fig_height: 5.5
    fig_caption: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
library(dplyr)
options(dplyr.print_max = 6, dplyr.print_min = 6)
options(width = 60)

library(dplyr)
library(readr)
library(ggplot2)
library(smodels)
library(forcats)
library(stringi)
library(ggrepel)

theme_set(theme_minimal())
```

##

![](img/data_pipeline_gather.png)

## web scraping

Today, we are going to use R to systemically build a dataset from
information on Wikipedia.

Words of caution about scraping data (note: this is not legal
advice):

- many websites prohibit the automatic downloading of material
in their terms and conditions
- a small set of people have gotten in serious trouble for scraping
websites against their terms of use
- we are only going to apply this to Wikipedia, as they have
generously permissive rules about this sort of thing
- if you ever want to replicate this on a large scale, please use
the Wikipedia API rather than the tools here

## largest cities

Wikipedia has curated a table of the most populous cities in the
world. We can download this list, as in the last class, using
the following code:

```{r}
url <- "https://en.wikipedia.org/wiki/List_of_largest_cities"
wpage <- data_frame(line = readLines(url))
```

Make sure to view the site itself as well in a browser.

## extracting cities

Searching through the source HTML, we see that rows in the
city table always start with the string `<th scope="row">`.
Let's filter our dataset to only include these rows:

```{r}
wpage <- filter(wpage, stri_detect(line, fixed = '<th scope="row">'))
```

## extracting city names

We will also remove all HTML tags on these rows, and store the results as
the variable city.

```{r}
wpage <- mutate(wpage, city = stri_replace_all(line, "", regex = "<[^>]+>"))
wpage$city
```

## extracting links

Finally, looking at row of data, notice that there is a link to the
Wikipedia page about each city:

```{r}
wpage$line[1]
```

## extracting links

We can extract these using the `stri_extract` function:

```{r}
wpage <- mutate(wpage, link = stri_extract(line, regex = "/wiki/[^[\"]]+"))
wpage$link
```

## a city page

With this link, we can now download the specific information
from a given city. For instance, let's take the 5'th city
(Lagos) and download the website from Wikipedia:

```{r}
i <- 5
url <- stri_c("https://en.wikipedia.org", wpage$link[i])
cpage <- data_frame(line = readLines(url))
```

Note the use of the `stri_c` function to combine the base URL
with the link.

## city coordinates

One piece of information on most city pages is the latitude and
longitude of the city. We can find this by detecting the string
"Coordinates:" in the webpage. Here, we save only the first
occurrence of the string (here there is only one, but we want
to safely abstract this to other pages):

```{r}
coord <- filter(cpage, stri_detect(line,
                          fixed = "Coordinates:"))$line[1]
coord
```

## city coordinates

Looking at the output, we want to not just remove the HTML tags but
to capture a very specific part of the tags. Here we get the easiest
form of the coordinates from the "geo" tag:

```{r}
coord <- stri_extract(coord,
                      regex = "<span class=\"geo\">[^<]+</span>")
coord
```

## city coordinates

We want to now remove the html tags. We could do this by a regular expression,
but let's instead do it by taking a substring:

```{r}
coord <- stri_sub(coord, 19, -8)
coord
```

## city coordinates

Next, we want to split the string into two parts based on the ";" symbol.
We do this with the `stri_split` function.

```{r}
coord <- stri_split(coord, fixed = ";")[[1]]
coord
```

## city coordinates

Finally, we want to convert these strings into numbers. Recall that
`factor` turns numeric data into categorical ones; at the time, I
mentioned that rarely can we go in the reverse order. Here we use
the `as.numeric` function to do just that:

```{r}
coord <- as.numeric(coord)
lat <- coord[1]
lon <- coord[2]
lat
lon
```

## country name

Let's also find the name of the country that each city is in.
this is a bit complicated because the row that shows the tag
"Country" is actually one away from the row that gives the
data:

```{r}
filter(cpage, stri_detect(line,
               fixed = "<th scope=\"row\">Country</th>"))
```

## country name

To fix this, wrap the `stri_detect` function in the function `which`. This
gives the indices where something is True:

```{r}
id <- which(stri_detect(cpage$line,
             fixed = "<th scope=\"row\">Country</th>"))
id
```

## country name

Now, we just add 1 to this index (extracting just the first one, in case there
are multiple) and grab those lines:

```{r}
country <- cpage$line[id[1] + 1]
country
```

## country name

Cleaning up the results yields the desired information:

```{r}
country <- cpage$line[which(stri_detect(cpage$line,
            fixed = "<th scope=\"row\">Country</th>"))[1] + 1]
country <- stri_replace_all(country, "", regex = "<[^>]+>")
country <- stri_replace_all(country, "", fixed = "&#160;")
country
```

## automating over cities

We now have code that works on a single city page. Let's add empty
attributes to the `wpage` dataset. We'll fill these in in a moment.

```{r}
wpage$lat <- NA
wpage$lon <- NA
wpage$country <- NA
```

## automating over cities

We now use a *for loop* to cycle over all rows of the `wpage` dataset
as follows:

```{r, eval = FALSE}
for (i in 1:nrow(wpage)) {
  url <- stri_c("https://en.wikipedia.org", wpage$link[i])
  cpage <- data_frame(line = readLines(url))

  # extract stuff

  wpage$lat[i] <- lat
  wpage$lon[i] <- lon
  wpage$country[i] <- country
}
```

## automating over cities

```{r, eval = FALSE}
for (i in 1:nrow(wpage)) {
  url <- stri_c("https://en.wikipedia.org", wpage$link[i])
  cpage <- data_frame(line = readLines(url))

  coord <- filter(cpage,
      stri_detect(line, fixed = "Coordinates:"))$line[1]
  coord <- stri_extract(coord,
      regex = "<span class=\"geo\">[^<]+</span>")
  coord <- stri_sub(coord, 19, -8)
  coord <- stri_split(coord, fixed = ";")[[1]]
  coord <- as.numeric(coord)
  wpage$lat[i] <- coord[1]
  wpage$lon[i] <- coord[2]

  country <- cpage$line[which(stri_detect(cpage$line,
      fixed = "<th scope=\"row\">Country</th>"))[1] + 1]
  country <- stri_replace_all(country, "", regex = "<[^>]+>")
  country <- stri_replace_all(country, "", fixed = "&#160;")

  wpage$country[i] <- country
}
```

## results

```{r, echo = FALSE, eval = TRUE, message = FALSE}
wpage <- read_csv("wpage_cities.csv")
```

We can now plot the data as follows:

```{r, eval = FALSE}
qplot(lon, lat, data = wpage, label = city, geom = "text") +
  theme_minimal()
```

## results

```{r, echo = FALSE, eval = TRUE, message = FALSE}
qplot(lon, lat, data = wpage, label = city, geom = "blank") +
  geom_text(size = 2.5) +
  theme_minimal()
```


## results

```{r, echo = FALSE, eval = TRUE, message = FALSE}
qplot(lon, lat, data = wpage, label = city, geom = "blank") +
  geom_point(aes(color = fct_lump(country, 4))) +
  theme_minimal()
```

## results

Or tabulate by country:

```{r}
count(wpage, country, sort = TRUE)
```


