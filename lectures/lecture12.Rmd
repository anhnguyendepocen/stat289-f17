---
title: "Lecture 12: Inference for the Mean"
author: "Taylor Arnold"
date: " "
fontsize: 11pt
output:
  beamer_presentation:
    template: template.beamer
    pandoc_args: "--latex-engine=xelatex"
    theme: "metropolis"
    fonttheme: "professionalfonts"
    slide_level: 2
    df_print: tibble
    fig_width: 9
    fig_height: 5.5
    fig_caption: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
library(dplyr)
options(dplyr.print_max = 6, dplyr.print_min = 6)
options(width = 60)

library(dplyr)
library(readr)
library(ggplot2)
library(smodels)
library(forcats)

theme_set(theme_minimal())

coins <- data_frame(number = c(1,2,2,3,4,4,4,5))
helicopter <- data_frame(flight_time = c(0.9, 1.11, 1.13, 0.92, 1.16, 1.11))
coins2 <- data_frame(number = c(1,1,4,5,1,3,4,4), cup = c(rep("A", 4), rep("B", 4)))
```

# One-way Inference

##

![](img/data_pipeline_model.png)

## inference

The first model we will look at is
modeling the mean value of some random process. In this handout
we will start to see how to implement this model directly in R and how
to analyze how well it fits the data at hand.

## a simple example

We want to take the observations of some numeric variable and provide
an estimate of its **true** mean. The techniques we
cover today will apply to two similar but conceptually different
cases. These are:

- a variable sampled independently from a larger population
- a variable observed from repeated trials of a random process

## a simple example

In the first case, the true mean is the mean of the entire population.
For the second case, the true mean is the average value we would get from
an infinite number of trials. As long as the sample from the population
is taken at random and the output collected from each random trial
is independent of prior trials, the same exact technique is used for
estimating the mean of both situations.

## a simple example

Consider a random sample of coins from a cup similar to the one we
have in class:

```{r}
coins
```

## a simple example

Our best guess for the average value of all of the coins in the cups
might be the mean of the sample we took:

```{r}
mean(coins$number)
```

## using lm_basic

Let's do this is a different way that will allow us to extrapolate
on this single number:

```{r}
model <- lm_basic(number ~ 1, data = coins)
```

This says to construct a model for the variable `number` from the data
set `coins`. The `1` indicates that we are fitting a
single mean to the dataset; we will see later how to fit more
complex models.

## using lm_basic

To see the output of the model, run `reg_table`:

```{r, eval = FALSE}
reg_table(model)
```

The model calls the mean an intercept, for reasons that will become
clear shortly, and it gives the exact same value as with our old
technique. The other numbers above and below the table can be useful
but are not our primary subject of interest at the moment.

## using lm_basic

\footnotesize

```{r, eval = TRUE}
reg_table(model)
```
## using lm_basic - level

Why bother with this more involved method for finding a mean? For
one thing, `reg_table` provides an option called `level` that can
be set to a number between 0 and 1. For example:

```{r, eval = FALSE}
reg_table(model, level = 0.9)
```

## using lm_basic - level

\footnotesize

```{r, eval = TRUE}
reg_table(model, level = 0.9)
```

## confidence interval

The table now includes two additional numbers of the mean: the
10th and 90th percentiles of a *confidence interval*. A confidence
interval provides a guess for where the true mean actually lies.

The construction of a confidence interval involves some surprisingly
deep mathematics, including the law of large numbers and the central
limit theorem.

Using confidence intervals is, however, incredibly simple! The
confidence level, here 90%, gives the probability that the testing
procedure will lead to a correct result if a sample or experiment
is repeated many times. Common confidence levels include 90%, 95%,
and 99%.

## a second example

Taking a set of sampled flight times from paper helicopters, we
can run the exact same analysis:

```{r, eval = FALSE}
model <- lm_basic(flight_time ~ 1, data = helicopter)
reg_table(model, level = 0.95)
```

Unless we have a specific reason to use a different level, we will
usually use a 95% confidence interval in this course.

## a second example

\footnotesize

```{r, eval = TRUE}
model <- lm_basic(flight_time ~ 1, data = helicopter)
reg_table(model, level = 0.95)
```

# Inference Across Groups

## coins2

The `lm_basic` function allows for much more complex models than
describing a simple mean. Consider a second set of data where
coins have been taken from two different cups:

```{r}
coins2
```

## lm_basic again

In this case, we may want to model the mean of both cups. To
do this with `lm_basic`, we just add the new variable to the
formula:

```{r, eval = FALSE}
model <- lm_basic(number ~ 1 + cup, data = coins2)
reg_table(model)
```

## lm_basic again

\footnotesize

```{r, eval = TRUE}
reg_table(lm_basic(number ~ 1 + cup, data = coins2))
```

## lm_basic again

How do we read this new table? The intercept gives the mean
value for the **A** cup and the second term, called a slope,
gives the additional amount needed to get the mean of the
coins from cup **B**. So, the best guess of cup B's mean is
equal to 3.

## lm_basic - confidence intervals

What does the table look like when we add confidence
intervals:

```{r, eval = FALSE}
reg_table(model, level = 0.95)
```

## lm_basic - confidence intervals

\footnotesize

```{r,}
reg_table(model, level = 0.95)
```

## lm_basic - confidence intervals

The mean of cup A is predicted to be between 0.5872 and 4.913.
The difference between cup B's mean and cup A's mean is somewhere
between -2.8086 and 3.309. Because this difference includes zero,
we say that there is no statistical evidence (at the 95% level)
that the mean of the two cups is different.

Think about this statement for a bit. Why would a value of zero
be important in this model?

## another example

Let's apply this to a more complex situation using the mammals sleep
dataset. We can model the average time spent awake as a function
of the diet type of a given mammal:

```{r, eval = FALSE}
model <- lm_basic(awake ~ 1 + vore, data = msleep)
reg_table(model, level = 0.95)
```

## another example

\footnotesize

```{r, eval = FALSE}
reg_table(lm_basic(awake ~ 1 + vore, data = msleep), level = 0.95)
```

## another example

Now each of these values gives the difference between the base level,
carnivores, and all of the others. So the predicted mean for hours
spent awake for insectivores is 13.6263 + (-4.5662), or about 9 hours.

The confidence intervals tell us whether there is evidence that a
given diet type is different from carnivores. We see, for example,
that there is statistical evidence that insectivores differ from
carnivores, but no evidence for distinctions between carinvores
and any other groups.

## the base level

The careful observer will notice that there is a problem with this
approach: what if we want to compare two values when one is not the
base level? To do so, use the `fct_relevel` command with the new
baseline used as the second parameter:

```{r, eval = TRUE}
msleep <- mutate(msleep, vore_new = fct_relevel(vore, "insecti"))
model <- lm_basic(awake ~ 1 + vore_new, data = msleep)
```

Now everything is compared to the insecti category.

## the base level

\footnotesize

```{r, eval = TRUE}
reg_table(model, level = 0.95)
```





